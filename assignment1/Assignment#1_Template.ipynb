{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIa-poEHfw5Z"
      },
      "source": [
        "#Assignment 1 (Random Forest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGU057DfgTIL"
      },
      "source": [
        "* In this project you are given a dataset of housing housing price prediction. \n",
        "Dataset description is found in the given datasets.\n",
        "\n",
        "* The goal of the project is to predict the price of a house given its attributes. \n",
        "Therefore, the problem is a regression task. \n",
        "\n",
        "* You need to build a random forest that consists of multiple decision trees (for regression) from the given training data set. Then, apply it on the test set and submit your code to generate predictions.\n",
        "You need to build the random forest and decision trees from scratch. (I.e., it is not allowed to use existing machine learning libraries or packages such as sklearn.)\n",
        "\n",
        "* You may use any programming language/environment of your choice, but you are required to submit the complete source code to produce the output\n",
        "If you use anything other than jupyter notebook, submit an executable and run that from the main function of the jupyter notebook so that the prediction generation is automated. We can provide assistance with this.\n",
        "The output (a single file with the predictions for each test instance) must be generated automatically using the approach implemented by you. Submitting predictions/code from any other source (Internet, another student, etc.) is considered cheating and will result in immediate disqualification (i.e., dismissal from the course)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kut1m20f4zR"
      },
      "source": [
        "##Part 1: Preprocessing and dataset analysis (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yANfSgagRJq"
      },
      "source": [
        "* The given dataset is quite complex, it has many attributes, and not all of them are useful! \n",
        "Training on such dataset results in a bad accuracy. And this is exactly the point! \n",
        "\n",
        "* \"Understanding the question is half the answer\". In data mining, understanding the dataset is half the answer! \n",
        "\n",
        "* In part 1 you need to analyze the dataset and make it clean. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4xjsBzYlfjq"
      },
      "source": [
        "###Load the dataset and explore (5 points)\n",
        "\n",
        "* Load the dataset, view the dataset and the shape of it, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUvifadOf4G-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWmPCu5hmAIj"
      },
      "source": [
        "### Clean the dataset (10 points)\n",
        "\n",
        "* We cannot train on a 'dirty' dataset! There are duplicated, Null, and missing values that you need to take care of!\n",
        "\n",
        "* Drop all columns which have null values >= 70 % and drop all rows which have null values >= 70 %.\n",
        "\n",
        "* You need to take care of categorial data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exQbzeH7mM-I"
      },
      "outputs": [],
      "source": [
        "def PreprocessingData(db):\n",
        "    \"\"\"\n",
        "        TODO\n",
        "        Show NULL values for each columns in dataset and thier percentage\n",
        "        Drop all columns which have null values >= 70 % \n",
        "        Drop all rows which have null values >= 70 %\n",
        "        Fill all missing values with the (mean, mode)\n",
        "\n",
        "    \"\"\"\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ8kz_a-mdNx"
      },
      "source": [
        "### Correlations! (5 points)\n",
        "\n",
        "* Now we have a clean dataset, but not all attributes are useful! \n",
        "\n",
        "* Display the corrlation between all features and the sales price. This will show you which feature affects sales price more. You may use *corr()* function. \n",
        "\n",
        "* Choose the most correlated features, and remove others. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irh60fo-nVtw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhnNAO-wnXM6"
      },
      "source": [
        "## Part 2: Decision Tree (45 points)\n",
        "#### Building a Decision Tree:\n",
        "A Decision tree consists of nodes connected by edges. A decision tree is typically, a binary tree, which has the following properties:\n",
        "- One node is marked as Root node\n",
        "- Every node other than the root has a parent node\n",
        "- Each node can have at most 2 child nodes (left edge & right edge)\n",
        "- Leaf node is the node which contains pure data or when we reach to the maximum depth \n",
        "\n",
        "To create the decision tree model for scratch you need to create two classes (a class for the node, for example \"class DecisionNode():\" and a class for Decision Tree model, for example \"class RegressionDecisionTree():\")\n",
        "\n",
        "\n",
        "1- DecisionNode class used to save some values for each node we do the spliting on it until we reach the leaf node\n",
        "so we will save the following values for the node:\n",
        "- feature: feature index.\n",
        "- threshold: the value we used to split the data on.\n",
        "- value: the average value for the leaf node.\n",
        "- True_Branch: if the condition is true.\n",
        "- False_Branch: if the condition is false."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYNFB7QY-SJK"
      },
      "outputs": [],
      "source": [
        "class DecisionNode():\n",
        "    def __init__(self, feature_idx=None, threshold=None, value=None, true_branch=None, false_branch=None):\n",
        "        self.feature_idx = feature_idx # index of the feature that is used\n",
        "        self.threshold = threshold     # threshold value for feature when making the decision\n",
        "        self.value = value # Average value if the node is a leaf in the tree\n",
        "        self.true_branch = true_branch # the node we go to if decision returns True\n",
        "        self.false_branch = false_branch # the node we go to if decision returns False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYpCv95F-SJK"
      },
      "source": [
        "# Decision Tree Class\n",
        "This Class consists the following functions:\n",
        "<ol>\n",
        "<li> <b>build_tree</b>: used to create the decision tree nodes</li> \n",
        "<li> <b>calc_variance_reduction</b> : measure the impurity by using variance reduction measure (like MSE) </li> \n",
        "the function takes three parameters (parentRec: the records for the target before split,and the left and right records after splitting. This function used to measure the impurity for each node and decide if we will split or not.\n",
        "<li> <b>majority_vote</b>: used to calculate values for the leaf nodes records which equal to the mean of these records.</li> \n",
        "<li><b>split_by_feature</b>: this function take the feature and the threshold and check if the feature is numerical so it split the records into two node (true which is the left edge and false which is the right edge)\n",
        "if the feature is categorical so it split where the values equal to the threshold</li>\n",
        "<li> <b>fit</b>: Used to train the dataset after spliting the data into two part x: features, y: target</li>\n",
        "<li><b>predict_value</b>: used to predict the value for each record, it is a recursive method to find the leaf node that corresponds to prediction\n",
        "<li><b>predict</b>: take all records for the test data and iterate into each record to predit the y(target) value and save the result into a prediction list. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSe6v4r1-SJK"
      },
      "outputs": [],
      "source": [
        "class RegressionDecisionTree():\n",
        "    # constructor\n",
        "    def __init__(self, min_VarianceReduction=1e-7, max_depth=5):        \n",
        "        self.root = None # root of this tree\n",
        "        self.min_VarianceReduction = min_VarianceReduction # minimum VarianceReduction to allow splitting\n",
        "        # used to stopping conductions\n",
        "        self.max_depth = max_depth # maximum depth the tree grows to\n",
        " \n",
        "\n",
        "    # used to create the decision tree nodes\n",
        "    def build_tree(self, X, y, current_depth=0):\n",
        "        # we will use decision dictionary to save the feature and the threshold we build the tree on \n",
        "        decision = None\n",
        "        # we will use subtrees dictionary to save the feature and the threshold we build the tree on \n",
        "        subtrees = None\n",
        "        largest_variance_Reduction = 0\n",
        "        # add y as last column of X\n",
        "        df = pd.concat((X, y), axis=1)\n",
        "        n_rows, n_features = X.shape\n",
        "        if current_depth <= self.max_depth:\n",
        "            # iterate through every feature\n",
        "            for feature_idx in range(n_features):\n",
        "                # values of that column\n",
        "                feature_values = X.iloc[:, feature_idx]                \n",
        "                unique_values = feature_values.unique()                \n",
        "                for threshold in unique_values:\n",
        "                    X_trueEdge, X_falseEdge = self.split_by_feature(df, feature_idx, threshold)\n",
        "                    if len(X_trueEdge) > 0 and len(X_falseEdge) > 0:\n",
        "                        y_true = X_trueEdge.iloc[:,-1]\n",
        "                        y_false = X_falseEdge.iloc[:,-1]                        \n",
        "                        # Calculate impurity\n",
        "                        VarianceRed = self.Calc_variance_reduction(y, y_true, y_false)\n",
        "                        # Keep track of which feature gave the largest information gain\n",
        "                        if VarianceRed > largest_variance_Reduction:\n",
        "                            largest_variance_Reduction = VarianceRed\n",
        "                            decision = {\"feature_idx\":feature_idx, \"threshold\":threshold}\n",
        "                            subtrees = {\"X_true\":X_trueEdge.iloc[:,:-1],\n",
        "                                        \"y_true\":y_true,\n",
        "                                        \"X_false\":X_falseEdge.iloc[:,:-1],\n",
        "                                        \"y_false\":y_false}\n",
        "\n",
        "        # we will construct new branch of tree if the variance_Reduction is larger than minimum variance_Reduction that we've defined\n",
        "        if largest_variance_Reduction > self.min_VarianceReduction:\n",
        "            true_branch = self.build_tree(subtrees[\"X_true\"], subtrees[\"y_true\"], current_depth+1)\n",
        "            false_branch = self.build_tree(subtrees[\"X_false\"], subtrees[\"y_false\"], current_depth+1)\n",
        "            return DecisionNode(feature_idx=decision[\"feature_idx\"], threshold=decision[\"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
        "\n",
        "        # at leaf node we calculate the mean for the records\n",
        "        leaf_value = self.majority_vote(y)\n",
        "        return DecisionNode(value=leaf_value)\n",
        "                        \n",
        "    # measure the impurity by using variance reduction measure (like MSE)\n",
        "    # left_edgeRec= True edge: where condition is true\n",
        "    # Right_edgeRec= False edge: where condition is false\n",
        "    def Calc_variance_reduction(self, parentRec, left_edgeRec, Right_edgeRec):   \n",
        "        # return the VarReduction = variance for parent - (Weight * var(leftEdge) + Weight * var(RightEdge)    \n",
        "        # TODO\n",
        "        pass\n",
        "        \n",
        "    \n",
        "    def majority_vote(self, Y): \n",
        "        # return the majority_vote for the leaf nodes \n",
        "        # TODO\n",
        "        pass\n",
        "        \n",
        "             \n",
        "    def split_by_feature(self, db, feature_idx, threshold):\n",
        "        # TODO\n",
        "        # split the data into left_edge & right_edge depends one specified feature and the threshold\n",
        "        # return left & right edges\n",
        "        pass\n",
        "\n",
        "    \n",
        "    # Used to train the dataset after spliting the data into x: features, y: target\n",
        "    def fit(self, X, y):\n",
        "        self.root = self.build_tree(X, y)\n",
        "\n",
        "\n",
        "    def predict_value(self, xTest, tree=None):\n",
        "        # recursive method to find the leaf node that corresponds to prediction\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    # to predict the value we need to pass the all records for features and we save the prediction for each records into a list\n",
        "    def predict(self, XTest):\n",
        "        y_pred = []\n",
        "        for idx, row in XTest.iterrows():           \n",
        "            y_pred.append(self.predict_value(row.values))\n",
        "        return y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9aqjovq-SJM"
      },
      "source": [
        "- To Check the Accuracy for our prediction we use CalcAccuracy function which take the actual values for the test dataset and the predicted values and apply the RMSE formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24p2WfE8-SJN"
      },
      "outputs": [],
      "source": [
        "def CalcAccuracy(Actual_Y, Predicted_y):\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_2Ve9v--SJN"
      },
      "source": [
        "- Build decision tree model\n",
        "- Fit the model\n",
        "- Predict the values from test data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dJKxolT-SJN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvzaxcPAoJsk"
      },
      "source": [
        "## Part 3: Random Forest (20 points)\n",
        "#### Random forest class\n",
        "- the Class consist of the following functions:\n",
        "<ul>\n",
        "    <li>Constructor: consists of the subset data (Training & Testing) dataset after preprocessing and a list of deciceion tree objects </li>    \n",
        "    <li>Subsampling: Bagging we will take random sample with replacement for the Training dataset </li>\n",
        "    <li>build_model: first make subsample for the training dataset, then split the data into featurespart(X) and targetpart(Y), then take 10 samples of the feature part, finally build the decision tree (fit), this function take the number of DT that we want to build</li>\n",
        "    <li>predict: take the test dataset and make the prediction for the target field in all the tree in the random forest then take the mean for the prediction in each tree, finally add the mean of prediction to a list of predition </li>\n",
        "\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a00zi12Q-SJO"
      },
      "outputs": [],
      "source": [
        "class RF(object):\n",
        "    def __init__(self):\n",
        "        self.Traindata = None  # training data set (loaded into memory)\n",
        "        self.Testdata = None  # Test data set for prediction        \n",
        "        self.trees = []  # list of decision trees \n",
        "          \n",
        "\n",
        "     # This function generate a subsample with replacement\n",
        "    def __subsampling(self, train_set, sample_size_ratio):       \n",
        "        #sample_number = round(len(train_set) * sample_size_ratio)\n",
        "        #return  train_set.sample(n = sample_number, replace=True)\n",
        "        # TODO\n",
        "        pass\n",
        "        \n",
        "        \n",
        "    def build_model(self, train_set, sample_size_ratio, number_of_trees):\n",
        "        for i in range(number_of_trees):\n",
        "            TrainingSample = self.__subsampling(train_set, sample_size_ratio)\n",
        "            # TODO\n",
        "            # build a  tree\n",
        "            # Train the tree\n",
        "            # Add the tree to the Treeslist\n",
        "        pass\n",
        "            \n",
        "               \n",
        "    def predict(self, test_set):\n",
        "        # TODO\n",
        "        # Predict for each item in the list \n",
        "        # Calculate the mean \n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITGUmaXc-SJO"
      },
      "source": [
        "### Create Random Forest\n",
        "\n",
        "* Create 10 Decision Tree in the randomforest\n",
        "* Train the random forest with the dataset\n",
        "* Use the created random forest to predict the test dataset "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SjqaGyyl-qoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWmfgEVp-SJO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVGAlCDkoRfo"
      },
      "source": [
        "##Part4: Comparison! (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have finished implementing your Random Forest, it's time for some experiments and analysis! \n",
        "\n",
        "* Use the Random Forest in the scikit-learn library and train it on the same dataset. \n",
        "\n",
        "* Compare the accuracy given by your Random Forest to the scikit-learn one. \n",
        "\n",
        "* Increase the number of trees in your Random Forest. Does it improve the accuracy? \n",
        "\n",
        "* Make a table for comparing your Random Forest accuracy with different number of trees with the scikit-learn one. What is your conclusion? "
      ],
      "metadata": {
        "id": "8tHF7qjiBl6A"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12 (tags/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "e42f74ecac23521ece3572bae462a7c7939cd558a6fbb59afafe09d03193ca58"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}