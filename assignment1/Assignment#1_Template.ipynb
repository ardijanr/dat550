{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIa-poEHfw5Z"
   },
   "source": [
    "#Assignment 1 (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGU057DfgTIL"
   },
   "source": [
    "* In this project you are given a dataset of housing housing price prediction. \n",
    "Dataset description is found in the given datasets.\n",
    "\n",
    "* The goal of the project is to predict the price of a house given its attributes. \n",
    "Therefore, the problem is a regression task. \n",
    "\n",
    "* You need to build a random forest that consists of multiple decision trees (for regression) from the given training data set. Then, apply it on the test set and submit your code to generate predictions.\n",
    "You need to build the random forest and decision trees from scratch. (I.e., it is not allowed to use existing machine learning libraries or packages such as sklearn.)\n",
    "\n",
    "* You may use any programming language/environment of your choice, but you are required to submit the complete source code to produce the output\n",
    "If you use anything other than jupyter notebook, submit an executable and run that from the main function of the jupyter notebook so that the prediction generation is automated. We can provide assistance with this.\n",
    "The output (a single file with the predictions for each test instance) must be generated automatically using the approach implemented by you. Submitting predictions/code from any other source (Internet, another student, etc.) is considered cheating and will result in immediate disqualification (i.e., dismissal from the course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kut1m20f4zR"
   },
   "source": [
    "##Part 1: Preprocessing and dataset analysis (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yANfSgagRJq"
   },
   "source": [
    "* The given dataset is quite complex, it has many attributes, and not all of them are useful! \n",
    "Training on such dataset results in a bad accuracy. And this is exactly the point! \n",
    "\n",
    "* \"Understanding the question is half the answer\". In data mining, understanding the dataset is half the answer! \n",
    "\n",
    "* In part 1 you need to analyze the dataset and make it clean. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4xjsBzYlfjq"
   },
   "source": [
    "###Load the dataset and explore (5 points)\n",
    "\n",
    "* Load the dataset, view the dataset and the shape of it, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AUvifadOf4G-"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# def parse_data_description():\n",
    "#     f = open(\"./housing_data_description.csv\",\"r\")\n",
    "\n",
    "\n",
    "#     line = \" \"\n",
    "#     datatypes = {}\n",
    "#     datatypes_description = {}\n",
    "#     # i = 0\n",
    "#     current_key = \"\"\n",
    "#     while line!=\"\":\n",
    "#         line = f.readline()\n",
    "#         if not line.startswith(\"       \"):\n",
    "#             content = line.strip()\n",
    "\n",
    "#             #emtpy line\n",
    "#             if content==\"\":\n",
    "#                 continue\n",
    "\n",
    "#             keyinfo = content.split(\":\")\n",
    "#             key, description = keyinfo[0].strip(),keyinfo[1].strip()\n",
    "\n",
    "#             current_key = key\n",
    "#             datatypes[key] = {\n",
    "#                 \"type_description\" : description\n",
    "#             }\n",
    "#             continue\n",
    "\n",
    "#         value_info = line.lstrip().rstrip()\n",
    "#         if value_info==\"\":\n",
    "#             continue\n",
    "\n",
    "#         value_info = value_info.split(\"\t\")\n",
    "\n",
    "#         datatypes[current_key][value_info[0].strip()] = value_info[1].strip()\n",
    "\n",
    "#     pprint.pprint(datatypes)\n",
    "#     pprint.pprint(datatypes_description)\n",
    "\n",
    "\n",
    "db = pd.read_csv('./housing_price_train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWmPCu5hmAIj"
   },
   "source": [
    "### Clean the dataset (10 points)\n",
    "\n",
    "* We cannot train on a 'dirty' dataset! There are duplicated, Null, and missing values that you need to take care of!\n",
    "\n",
    "* Drop all columns which have null values >= 70 % and drop all rows which have null values >= 70 %.\n",
    "\n",
    "* You need to take care of categorial data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "exQbzeH7mM-I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  MSSubClass MSZoning  LotArea Street LotShape LandContour  \\\n",
      "0        1          60       RL     8450   Pave      Reg         Lvl   \n",
      "1        2          20       RL     9600   Pave      Reg         Lvl   \n",
      "2        3          60       RL    11250   Pave      IR1         Lvl   \n",
      "3        4          70       RL     9550   Pave      IR1         Lvl   \n",
      "4        5          60       RL    14260   Pave      IR1         Lvl   \n",
      "...    ...         ...      ...      ...    ...      ...         ...   \n",
      "1455  1456          60       RL     7917   Pave      Reg         Lvl   \n",
      "1456  1457          20       RL    13175   Pave      Reg         Lvl   \n",
      "1457  1458          70       RL     9042   Pave      Reg         Lvl   \n",
      "1458  1459          20       RL     9717   Pave      Reg         Lvl   \n",
      "1459  1460          20       RL     9937   Pave      Reg         Lvl   \n",
      "\n",
      "     Utilities LotConfig LandSlope  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
      "0       AllPub    Inside       Gtl  ...             0         0           0   \n",
      "1       AllPub       FR2       Gtl  ...             0         0           0   \n",
      "2       AllPub    Inside       Gtl  ...             0         0           0   \n",
      "3       AllPub    Corner       Gtl  ...           272         0           0   \n",
      "4       AllPub       FR2       Gtl  ...             0         0           0   \n",
      "...        ...       ...       ...  ...           ...       ...         ...   \n",
      "1455    AllPub    Inside       Gtl  ...             0         0           0   \n",
      "1456    AllPub    Inside       Gtl  ...             0         0           0   \n",
      "1457    AllPub    Inside       Gtl  ...             0         0           0   \n",
      "1458    AllPub    Inside       Gtl  ...           112         0           0   \n",
      "1459    AllPub    Inside       Gtl  ...             0         0           0   \n",
      "\n",
      "     PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
      "0           0       0       2    2008        WD         Normal    208500  \n",
      "1           0       0       5    2007        WD         Normal    181500  \n",
      "2           0       0       9    2008        WD         Normal    223500  \n",
      "3           0       0       2    2006        WD        Abnorml    140000  \n",
      "4           0       0      12    2008        WD         Normal    250000  \n",
      "...       ...     ...     ...     ...       ...            ...       ...  \n",
      "1455        0       0       8    2007        WD         Normal    175000  \n",
      "1456        0       0       2    2010        WD         Normal    210000  \n",
      "1457        0    2500       5    2010        WD         Normal    266500  \n",
      "1458        0       0       4    2010        WD         Normal    142125  \n",
      "1459        0       0       6    2008        WD         Normal    147500  \n",
      "\n",
      "[1460 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "def PreprocessingData(db: pd.DataFrame):\n",
    "    \"\"\"\n",
    "        TODO\n",
    "        Show NULL values for each columns in dataset and thier percentage\n",
    "        Drop all columns which have null values >= 70 %\n",
    "        Drop all rows which have null values >= 70 %\n",
    "        Fill all missing values with the (mean, mode)\n",
    "    \"\"\"\n",
    "\n",
    "    for index, row in db.iterrows():\n",
    "        row_null_percentage = row.isnull().sum()/db.shape[0]\n",
    "        if row_null_percentage >= 0.7:\n",
    "            db.drop(index=index, inplace=True)\n",
    "\n",
    "    for column_name in db.columns:\n",
    "        column_null_percentage = db[column_name].isnull().sum()/db.shape[1]\n",
    "        if column_null_percentage >= 0.7:\n",
    "            db.drop(columns=column_name,inplace=True)\n",
    "\n",
    "    value_types  = [\n",
    "        \"LotFrontage\",\n",
    "        \"LotArea\",\n",
    "        \"BsmtFinSF1\",\n",
    "        \"BsmtFinSF2\",\n",
    "        \"BsmtUnfSF\",\n",
    "        \"TotalBsmtSF\",\n",
    "        \"1stFlrSF\",\n",
    "        \"2ndFlrSF\",\n",
    "        \"LowQualFinSF\",\n",
    "        \"GrLivArea\",\n",
    "        \"BsmtFullBath\",\n",
    "        \"BsmtHalfBath\",\n",
    "        \"FullBath\",\n",
    "        \"HalfBath\",\n",
    "        \"Bedroom\",\n",
    "        \"Kitchen\",\n",
    "        \"TotRmsAbvGrd\",\n",
    "        \"Fireplaces\",\n",
    "        \"GarageYrBlt\",\n",
    "        \"GarageCars\",\n",
    "        \"GarageArea\",\n",
    "        \"WoodDeckSF\",\n",
    "        \"OpenPorchSF\",\n",
    "        \"EnclosedPorch\",\n",
    "        \"3SsnPorch\",\n",
    "        \"ScreenPorch\",\n",
    "        \"PoolArea\",\n",
    "        \"MiscVal\",\n",
    "    ]\n",
    "\n",
    "    # Fill any columns of value types with the mean\n",
    "    for column_name in value_types:\n",
    "        try: # Key might have been removed due to the step above\n",
    "            if db[column_name].isnull().any():\n",
    "                db[column_name].fillna(db[column_name].mean(), inplace=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # Fill any categorical data with the mode() value, meaning the value which\n",
    "    # appears the the most\n",
    "    db.fillna(db.mode().iloc[0], inplace=True)\n",
    "\n",
    "PreprocessingData(db)\n",
    "\n",
    "print(db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQ8kz_a-mdNx"
   },
   "source": [
    "### Correlations! (5 points)\n",
    "\n",
    "* Now we have a clean dataset, but not all attributes are useful! \n",
    "\n",
    "* Display the corrlation between all features and the sales price. This will show you which feature affects sales price more. You may use *corr()* function. \n",
    "\n",
    "* Choose the most correlated features, and remove others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Irh60fo-nVtw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalePrice        1.000000\n",
      "OverallQual      0.790982\n",
      "GrLivArea        0.708624\n",
      "GarageCars       0.640409\n",
      "GarageArea       0.623431\n",
      "TotalBsmtSF      0.613581\n",
      "1stFlrSF         0.605852\n",
      "FullBath         0.560664\n",
      "TotRmsAbvGrd     0.533723\n",
      "YearBuilt        0.522897\n",
      "YearRemodAdd     0.507101\n",
      "MasVnrArea       0.472614\n",
      "Fireplaces       0.466929\n",
      "BsmtFinSF1       0.386420\n",
      "WoodDeckSF       0.324413\n",
      "2ndFlrSF         0.319334\n",
      "OpenPorchSF      0.315856\n",
      "HalfBath         0.284108\n",
      "LotArea          0.263843\n",
      "BsmtFullBath     0.227122\n",
      "BsmtUnfSF        0.214479\n",
      "BedroomAbvGr     0.168213\n",
      "ScreenPorch      0.111447\n",
      "PoolArea         0.092404\n",
      "MoSold           0.046432\n",
      "3SsnPorch        0.044584\n",
      "BsmtFinSF2      -0.011378\n",
      "BsmtHalfBath    -0.016844\n",
      "MiscVal         -0.021190\n",
      "Id              -0.021917\n",
      "LowQualFinSF    -0.025606\n",
      "YrSold          -0.028923\n",
      "OverallCond     -0.077856\n",
      "MSSubClass      -0.084284\n",
      "EnclosedPorch   -0.128578\n",
      "KitchenAbvGr    -0.135907\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11898/1020346835.py:1: FutureWarning: The default value of numeric_only in DataFrame.corrwith is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  sale_price_correlation = db.corrwith(db['SalePrice']).sort_values(ascending=False)\n"
     ]
    }
   ],
   "source": [
    "sale_price_correlation = db.corrwith(db['SalePrice']).sort_values(ascending=False)\n",
    "print(sale_price_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',\n",
      "       'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt',\n",
      "       'YearRemodAdd'],\n",
      "      dtype='object')\n",
      "      SalePrice  OverallQual  GrLivArea  GarageCars  GarageArea  TotalBsmtSF  \\\n",
      "0        208500            7       1710           2         548          856   \n",
      "1        181500            6       1262           2         460         1262   \n",
      "2        223500            7       1786           2         608          920   \n",
      "3        140000            7       1717           3         642          756   \n",
      "4        250000            8       2198           3         836         1145   \n",
      "...         ...          ...        ...         ...         ...          ...   \n",
      "1455     175000            6       1647           2         460          953   \n",
      "1456     210000            6       2073           2         500         1542   \n",
      "1457     266500            7       2340           1         252         1152   \n",
      "1458     142125            5       1078           1         240         1078   \n",
      "1459     147500            5       1256           1         276         1256   \n",
      "\n",
      "      1stFlrSF  FullBath  TotRmsAbvGrd  YearBuilt  YearRemodAdd  \n",
      "0          856         2             8       2003          2003  \n",
      "1         1262         2             6       1976          1976  \n",
      "2          920         2             6       2001          2002  \n",
      "3          961         1             7       1915          1970  \n",
      "4         1145         2             9       2000          2000  \n",
      "...        ...       ...           ...        ...           ...  \n",
      "1455       953         2             7       1999          2000  \n",
      "1456      2073         2             7       1978          1988  \n",
      "1457      1188         2             9       1941          2006  \n",
      "1458      1078         1             5       1950          1996  \n",
      "1459      1256         1             6       1965          1965  \n",
      "\n",
      "[1460 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "sale_price_correlation_filtered = sale_price_correlation[sale_price_correlation > 0.50].index\n",
    "db_filtered = db[sale_price_correlation_filtered]\n",
    "\n",
    "print(sale_price_correlation_filtered)\n",
    "print(db_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhnNAO-wnXM6"
   },
   "source": [
    "## Part 2: Decision Tree (45 points)\n",
    "#### Building a Decision Tree:\n",
    "A Decision tree consists of nodes connected by edges. A decision tree is typically, a binary tree, which has the following properties:\n",
    "- One node is marked as Root node\n",
    "- Every node other than the root has a parent node\n",
    "- Each node can have at most 2 child nodes (left edge & right edge)\n",
    "- Leaf node is the node which contains pure data or when we reach to the maximum depth \n",
    "\n",
    "To create the decision tree model for scratch you need to create two classes (a class for the node, for example \"class DecisionNode():\" and a class for Decision Tree model, for example \"class RegressionDecisionTree():\")\n",
    "\n",
    "\n",
    "1- DecisionNode class used to save some values for each node we do the spliting on it until we reach the leaf node\n",
    "so we will save the following values for the node:\n",
    "- feature: feature index.\n",
    "- threshold: the value we used to split the data on.\n",
    "- value: the average value for the leaf node.\n",
    "- True_Branch: if the condition is true.\n",
    "- False_Branch: if the condition is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYNFB7QY-SJK"
   },
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    def __init__(self, feature_idx=None, threshold=None, value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_idx = feature_idx # index of the feature that is used\n",
    "        self.threshold = threshold     # threshold value for feature when making the decision\n",
    "        self.value = value # Average value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch # the node we go to if decision returns True\n",
    "        self.false_branch = false_branch # the node we go to if decision returns False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYpCv95F-SJK"
   },
   "source": [
    "# Decision Tree Class\n",
    "This Class consists the following functions:\n",
    "<ol>\n",
    "<li> <b>build_tree</b>: used to create the decision tree nodes</li> \n",
    "<li> <b>calc_variance_reduction</b> : measure the impurity by using variance reduction measure (like MSE) </li> \n",
    "the function takes three parameters (parentRec: the records for the target before split,and the left and right records after splitting. This function used to measure the impurity for each node and decide if we will split or not.\n",
    "<li> <b>majority_vote</b>: used to calculate values for the leaf nodes records which equal to the mean of these records.</li> \n",
    "<li><b>split_by_feature</b>: this function take the feature and the threshold and check if the feature is numerical so it split the records into two node (true which is the left edge and false which is the right edge)\n",
    "if the feature is categorical so it split where the values equal to the threshold</li>\n",
    "<li> <b>fit</b>: Used to train the dataset after spliting the data into two part x: features, y: target</li>\n",
    "<li><b>predict_value</b>: used to predict the value for each record, it is a recursive method to find the leaf node that corresponds to prediction\n",
    "<li><b>predict</b>: take all records for the test data and iterate into each record to predit the y(target) value and save the result into a prediction list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSe6v4r1-SJK"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class RegressionDecisionTree():\n",
    "    # constructor\n",
    "    def __init__(self, min_VarianceReduction=1e-7, max_depth=5):\n",
    "        self.root = None # root of this tree\n",
    "        self.min_VarianceReduction = min_VarianceReduction # minimum VarianceReduction to allow splitting\n",
    "        # used to stopping conductions\n",
    "        self.max_depth = max_depth # maximum depth the tree grows to\n",
    "\n",
    "\n",
    "    # used to create the decision tree nodes\n",
    "    # X is the feature matrix containing training data\n",
    "    # y is the stuff we want to predict, aka sales price\n",
    "    def build_tree(self, X:pd.DataFrame, y: pd.DataFrame, current_depth=0):\n",
    "        # we will use decision dictionary to save the feature and the threshold we build the tree on\n",
    "        decision = None\n",
    "        # we will use subtrees dictionary to save the feature and the threshold we build the tree on\n",
    "        subtrees = None\n",
    "        largest_variance_Reduction = 0\n",
    "        # add y as last column of X\n",
    "        df = pd.concat((X, y), axis=1)\n",
    "        n_rows, n_features = X.shape\n",
    "        if current_depth <= self.max_depth:\n",
    "            # iterate through every feature\n",
    "            for feature_idx in range(n_features):\n",
    "                # values of that column\n",
    "                feature_values = X.iloc[:, feature_idx]\n",
    "                unique_values = feature_values.unique()\n",
    "                for threshold in unique_values:\n",
    "                    X_trueEdge, X_falseEdge = self.split_by_feature(df, feature_idx, threshold)\n",
    "                    if len(X_trueEdge) > 0 and len(X_falseEdge) > 0:\n",
    "                        y_true = X_trueEdge.iloc[:,-1]\n",
    "                        y_false = X_falseEdge.iloc[:,-1]\n",
    "                        # Calculate impurity\n",
    "                        VarianceRed = self.Calc_variance_reduction(y, y_true, y_false)\n",
    "                        # Keep track of which feature gave the largest information gain\n",
    "                        if VarianceRed > largest_variance_Reduction:\n",
    "                            largest_variance_Reduction = VarianceRed\n",
    "                            decision = {\n",
    "                                \"feature_idx\":feature_idx,\n",
    "                                \"threshold\":threshold\n",
    "                                }\n",
    "                            subtrees = {\n",
    "                                \"X_true\":X_trueEdge.iloc[:,:-1],\n",
    "                                \"y_true\":y_true,\n",
    "                                \"X_false\":X_falseEdge.iloc[:,:-1],\n",
    "                                \"y_false\":y_false\n",
    "                                }\n",
    "\n",
    "        # we will construct new branch of tree if the variance_Reduction is larger than minimum variance_Reduction that we've defined\n",
    "        if largest_variance_Reduction > self.min_VarianceReduction:\n",
    "            true_branch = self.build_tree(subtrees[\"X_true\"], subtrees[\"y_true\"], current_depth+1)\n",
    "            false_branch = self.build_tree(subtrees[\"X_false\"], subtrees[\"y_false\"], current_depth+1)\n",
    "            return DecisionNode(feature_idx=decision[\"feature_idx\"], threshold=decision[\"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "        # at leaf node we calculate the mean for the records\n",
    "        leaf_value = self.majority_vote(y)\n",
    "        return DecisionNode(value=leaf_value)\n",
    "\n",
    "    # measure the impurity by using variance reduction measure (like MSE)\n",
    "    # left_edgeRec= True edge: where condition is true\n",
    "    # Right_edgeRec= False edge: where condition is false\n",
    "    def Calc_variance_reduction(self, parentRec, left_edgeRec, Right_edgeRec):\n",
    "        # return the VarReduction = variance for parent - (Weight * var(leftEdge) + Weight * var(RightEdge)\n",
    "        n_left = len(left_edgeRec)\n",
    "        n_right = len(Right_edgeRec)\n",
    "        N = len(parentRec)\n",
    "\n",
    "        return parentRec.var() - (n_left/N) * left_edgeRec.var() + (n_right/N) * Right_edgeRec.var()\n",
    "\n",
    "\n",
    "    def majority_vote(self, Y):\n",
    "        Y.mean()\n",
    "\n",
    "\n",
    "    def split_by_feature(self, db, feature_idx, threshold):\n",
    "\n",
    "        #split_by_feature</b>:\n",
    "        # This function takes a feature and a threshold\n",
    "        # then splits the records into two nodes based on whether the feature is numerical or categorical.\n",
    "        # 1. For numerical features, it divides the data into a \"true\" node (left edge) for values less than or equal to the threshold,\n",
    "        # and a \"false\" node (right edge) for values greater than the threshold.\n",
    "        #\n",
    "        # 2. For categorical features, it splits the data into two nodes where one contains records with values equal to\n",
    "        # the threshold (true node), and the other contains all other values (false node).\n",
    "\n",
    "        # split the data into left_edge & right_edge depends one specified feature and the threshold\n",
    "        # return left & right edges\n",
    "\n",
    "        values = db[feature_idx]\n",
    "\n",
    "        if values[0].isnumeric():\n",
    "            left_true = values[values<=threshold]\n",
    "            right_false = values[values>threshold]\n",
    "\n",
    "            return (left_true, right_false)\n",
    "\n",
    "        left_true = values[values==threshold]\n",
    "        right_false = values[values!=threshold]\n",
    "\n",
    "        return (left_true, right_false)\n",
    "\n",
    "\n",
    "    # Used to train the dataset after spliting the data into x: features, y: target\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "\n",
    "    def predict_value(self, xTest, tree=None):\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        if hasattr(tree, 'value'):\n",
    "            return tree.value\n",
    "\n",
    "        feature_value = xTest[tree.feature_idx]\n",
    "\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self.predict_value(xTest, tree.true_branch)\n",
    "        else:\n",
    "            return self.predict_value(xTest, tree.false_branch)\n",
    "\n",
    "    # to predict the value we need to pass the all records for features and we save the prediction for each records into a list\n",
    "    def predict(self, XTest):\n",
    "        y_pred = []\n",
    "        for idx, row in XTest.iterrows():\n",
    "            y_pred.append(self.predict_value(row.values))\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9aqjovq-SJM"
   },
   "source": [
    "- To Check the Accuracy for our prediction we use CalcAccuracy function which take the actual values for the test dataset and the predicted values and apply the RMSE formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24p2WfE8-SJN"
   },
   "outputs": [],
   "source": [
    "def CalcAccuracy(Actual_Y, Predicted_y):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_2Ve9v--SJN"
   },
   "source": [
    "- Build decision tree model\n",
    "- Fit the model\n",
    "- Predict the values from test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dJKxolT-SJN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvzaxcPAoJsk"
   },
   "source": [
    "## Part 3: Random Forest (20 points)\n",
    "#### Random forest class\n",
    "- the Class consist of the following functions:\n",
    "<ul>\n",
    "    <li>Constructor: consists of the subset data (Training & Testing) dataset after preprocessing and a list of deciceion tree objects </li>    \n",
    "    <li>Subsampling: Bagging we will take random sample with replacement for the Training dataset </li>\n",
    "    <li>build_model: first make subsample for the training dataset, then split the data into featurespart(X) and targetpart(Y), then take 10 samples of the feature part, finally build the decision tree (fit), this function take the number of DT that we want to build</li>\n",
    "    <li>predict: take the test dataset and make the prediction for the target field in all the tree in the random forest then take the mean for the prediction in each tree, finally add the mean of prediction to a list of predition </li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a00zi12Q-SJO"
   },
   "outputs": [],
   "source": [
    "class RF(object):\n",
    "    def __init__(self):\n",
    "        self.Traindata = None  # training data set (loaded into memory)\n",
    "        self.Testdata = None  # Test data set for prediction\n",
    "        self.trees = []  # list of decision trees\n",
    "\n",
    "\n",
    "     # This function generate a subsample with replacement\n",
    "    def __subsampling(self, train_set, sample_size_ratio):\n",
    "        #sample_number = round(len(train_set) * sample_size_ratio)\n",
    "        #return  train_set.sample(n = sample_number, replace=True)\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "    def build_model(self, train_set, sample_size_ratio, number_of_trees):\n",
    "        for i in range(number_of_trees):\n",
    "            TrainingSample = self.__subsampling(train_set, sample_size_ratio)\n",
    "            # TODO\n",
    "            # build a  tree\n",
    "            # Train the tree\n",
    "            # Add the tree to the Treeslist\n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict(self, test_set):\n",
    "        # TODO\n",
    "        # Predict for each item in the list\n",
    "        # Calculate the mean\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITGUmaXc-SJO"
   },
   "source": [
    "### Create Random Forest\n",
    "\n",
    "* Create 10 Decision Tree in the randomforest\n",
    "* Train the random forest with the dataset\n",
    "* Use the created random forest to predict the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjqaGyyl-qoY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWmfgEVp-SJO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVGAlCDkoRfo"
   },
   "source": [
    "# Part4: Comparison! (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tHF7qjiBl6A"
   },
   "source": [
    "Now that you have finished implementing your Random Forest, it's time for some experiments and analysis! \n",
    "\n",
    "* Use the Random Forest in the scikit-learn library and train it on the same dataset. \n",
    "\n",
    "* Compare the accuracy given by your Random Forest to the scikit-learn one. \n",
    "\n",
    "* Increase the number of trees in your Random Forest. Does it improve the accuracy? \n",
    "\n",
    "* Make a table for comparing your Random Forest accuracy with different number of trees with the scikit-learn one. What is your conclusion? "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python-dat550 kernel",
   "language": "python",
   "name": "python-dat550"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e42f74ecac23521ece3572bae462a7c7939cd558a6fbb59afafe09d03193ca58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
