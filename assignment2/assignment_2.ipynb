{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pb\n",
    "import random\n",
    "import collections\n",
    "import string\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from scipy.sparse import csr_matrix\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czjcahcys1yO"
   },
   "source": [
    "# Assignment #2\n",
    "\n",
    "# In This assignment you are asked to read a data which include 48505 articles (Documents). Then find the most similar documents using Locality Sensitive Hashing. Follow the lecture covering this topic step by step.\n",
    "\n",
    "## 1. Data is available in Json format and you need to read it. 'https://www.ux.uis.no/~vsetty/data/assignment2_aricles.json' (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get('https://www.ux.uis.no/~vsetty/data/assignment2_aricles.json').json()[0:100]\n",
    "\n",
    "df = pb.DataFrame.from_dict(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shingle the documents (10 points)\n",
    "### Tips:\n",
    "* Use string package to cleanup the articles e.g, str.maketrans('', '', string.\n",
    "punctuation)\n",
    "* It is better to convert text to lower case that way you get fewer n-grams\n",
    "* apply ngrams(x.split(), n) using ngrams from nltk on the content + title for computing n-grams, for this data n = 2 is suffcient\n",
    "  * You can use n-gram at word level for this task\n",
    "  * try with different n-gram values \n",
    "  * You can use ngrams from nltk for this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(s):\n",
    "    t = str.maketrans('', '', string.punctuation+\"“”‘’\")\n",
    "    return s.translate(t).lower()\n",
    "\n",
    "def ngram_text(s):\n",
    "    n = ngrams(nltk.word_tokenize(s), 2)\n",
    "    c = collections.Counter(n)\n",
    "    m=c.most_common(10_000)\n",
    "    return [ w[0] for w in m]\n",
    "\n",
    "\n",
    "\n",
    "df['Title'] =  df['Title'].apply(cleanup)\n",
    "df['Content'] =  df['Content'].apply(cleanup)\n",
    "\n",
    "\n",
    "text_df = pb.DataFrame({})\n",
    "text_df[\"Article\"] = df['Title']+\" \"+df['Content']\n",
    "\n",
    "shingles_all_articles = pb.DataFrame({})\n",
    "shingles_all_articles[\"Shingles\"] = [text_df['Article'].str.cat(sep=' ')]\n",
    "\n",
    "shingles_all_articles = shingles_all_articles[\"Shingles\"].apply(ngram_text)[0]\n",
    "\n",
    "shingles_each_article = pb.DataFrame({})\n",
    "shingles_each_article[\"Shingles\"] = text_df['Article'].apply(ngram_text)\n",
    "\n",
    "\n",
    "#print(shingles_all_articles)\n",
    "#print(shingles_each_article)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Convert n-grams into binary vector representation for each document. You can do some optimzations if the matrix is too big. (10 points)\n",
    "* For example,\n",
    "\n",
    "  * Select top 10000 most frequent n-grams.\n",
    "  * You may also try smaller values of n (like 2 or 3) which result in fewer n-grams.\n",
    "  * Finally, you can also try sparse matrix representation. Like csr_matrix from scipy.sparse. It works even with full vocabulary.\n",
    "    * Given a list of n-grams for each document, see how to builid a sparse matrix here https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_binary_vec(article_shingles,all_shingles):\n",
    "    column, row = [], []\n",
    "\n",
    "    for c, article_ngrams in article_shingles['Shingles'].items():\n",
    "        article_ngrams = set(article_ngrams)\n",
    "        for r in range(len(all_shingles)):\n",
    "            if all_shingles[r] in article_ngrams:\n",
    "                column.append(c)\n",
    "                row.append(r)\n",
    "\n",
    "\n",
    "    return csr_matrix((len(row)*[1], (row, column)),shape=(len(all_shingles),len(article_shingles[\"Shingles\"])))\n",
    "\n",
    "\n",
    "matrix = to_binary_vec(shingles_each_article,shingles_all_articles)\n",
    "\n",
    "matrix = pb.DataFrame(matrix.todense())\n",
    "matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. We need hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers 0 through k − 1. It might be impossible to avoid collisions but as long as the collions are too many it won't matter much. (10 points)\n",
    "* The simplest would be using the builtin hash() function, it can be for example, hash(rownumber) % Numberofbuckets\n",
    "* You can generate several of these hash functions by xoring a random integer (hash(rownumber)^randint) % Numberofbuckets\n",
    "* It can also be a as simple as (rownumber * randint) % Numberofbuckets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "\n",
    "\n",
    "def make_hash_func():\n",
    "    a = list(range(1,10_001))\n",
    "    random.shuffle(a)\n",
    "    return a\n",
    "\n",
    "hash_funcs = []\n",
    "for _ in range(20):\n",
    "    hash_funcs.append(make_hash_func())\n",
    "    \n",
    "pb.DataFrame(hash_funcs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_matrix(row):\n",
    "    signature = []\n",
    "    for func in hash_funcs:\n",
    "        for i in range(1,10_001):\n",
    "            if row[func.index(i)] == 1:               \n",
    "                signature.append(i)\n",
    "                break\n",
    "    return signature\n",
    "\n",
    "signed_matrixes = matrix.apply(hash_matrix,axis=0)\n",
    "signed_matrixes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute minhash following the faster algorithm from the lecture (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0    1    2    3    4    5    6    7    8    9   ...   90   91   92   93  \\\n",
      "0  174  152   45   52  183  166  119   14   96   88  ...   24  167   30   26   \n",
      "1   19  191   45   82  167   66  150   79  172   14  ...  154  170   63  182   \n",
      "2   52  147    8    4   61   99  141  148   58  173  ...  161  179  139  132   \n",
      "3  139  144  184  183  153   72   55  128   34  187  ...  170  159   91  191   \n",
      "4   89   20  134  129  129   32   67  161   98    5  ...  150  127  172  177   \n",
      "5  112  112   61    7  161  100  158   28   60    4  ...   13  107   37   48   \n",
      "6  161    8  102   56  159   23  154  111   85   52  ...  139  154   88  190   \n",
      "7   96   41   13   96  170  193   29   50   69   55  ...  185   24  184  103   \n",
      "8   62  141  167  119  198   50   95  184  164  131  ...   48   80   38   36   \n",
      "9  157  123   63  145  111  123   36  128  137   59  ...   95   37  123   69   \n",
      "\n",
      "    94   95   96   97   98   99  \n",
      "0  188  158  172   67   34    3  \n",
      "1   24   90   82  165  132  114  \n",
      "2   66   57    8  193   22   19  \n",
      "3   31  187   34   32   21  193  \n",
      "4   91  146  146   29  192   28  \n",
      "5   39   95    7  127    7  189  \n",
      "6  180  166  137  142   23   76  \n",
      "7    7  136  199  167  171  148  \n",
      "8   92  186  166   19  191   91  \n",
      "9  128  111    1   50  188  114  \n",
      "\n",
      "[10 rows x 100 columns]\n",
      "0 -> {33, 26, 75}\n",
      "1 -> {96, 50, 83, 60, 62}\n",
      "2 -> {39, 72, 51, 52, 23, 59}\n",
      "3 -> {40, 99, 37}\n",
      "4 -> {9, 3, 77}\n",
      "5 -> {9, 74, 20, 62}\n",
      "6 -> {75, 28, 30, 55}\n",
      "7 -> {96, 98, 3, 72, 74, 51, 23, 94}\n",
      "8 -> {96, 1, 2, 64, 68, 44}\n",
      "9 -> {69, 86}\n",
      "10 -> {42, 75, 81, 84, 55}\n",
      "11 -> {32, 75, 29}\n",
      "12 -> {73, 10, 16, 84, 24}\n",
      "13 -> {2, 26, 78, 80, 57, 90}\n",
      "14 -> {9, 10, 7}\n",
      "15 -> {85, 38}\n",
      "16 -> {41, 79}\n",
      "17 -> {57, 27, 76}\n",
      "18 -> {13, 16, 23, 24, 28}\n",
      "19 -> {0, 97, 34, 99, 15, 47, 82}\n",
      "20 -> {1, 37, 70, 72, 73, 17, 53, 54, 57}\n",
      "21 -> {98, 76, 80, 81, 83, 21, 23}\n",
      "22 -> {98, 34, 72, 73, 74, 48, 83, 20}\n",
      "23 -> {98, 67, 5}\n",
      "24 -> {81, 90, 91, 94}\n",
      "25 -> {67, 11, 76, 44, 83, 53, 23, 63}\n",
      "26 -> {40, 93}\n",
      "28 -> {99, 7}\n",
      "29 -> {97, 35, 6, 41, 46, 79, 31}\n",
      "30 -> {67, 92, 38, 79}\n",
      "31 -> {16, 17, 94, 47}\n",
      "32 -> {97, 5, 67, 29}\n",
      "33 -> {48, 73, 88}\n",
      "34 -> {96, 98, 37, 8, 47, 23, 22, 87, 27}\n",
      "36 -> {6, 71, 47, 86, 93, 31}\n",
      "37 -> {33, 69, 73, 42, 84, 25, 26, 91, 92}\n",
      "38 -> {48, 20, 92, 87}\n",
      "39 -> {16, 33, 58, 94}\n",
      "40 -> {65, 82, 18, 34}\n",
      "41 -> {1, 10, 43}\n",
      "42 -> {34, 66, 45, 58, 61}\n",
      "43 -> {13, 23}\n",
      "44 -> {81, 10, 69}\n",
      "45 -> {32, 65, 2, 35, 45, 80, 18, 82}\n",
      "46 -> {44, 36}\n",
      "47 -> {80, 52, 37, 36}\n",
      "48 -> {71, 16, 87, 90, 93, 31}\n",
      "49 -> {37, 54}\n",
      "50 -> {97, 36, 5, 7, 22, 60}\n",
      "51 -> {11, 46, 80, 55, 62}\n",
      "52 -> {0, 3, 9, 12, 15, 58}\n",
      "53 -> {84, 63, 71}\n",
      "54 -> {72}\n",
      "55 -> {66, 67, 70, 6, 9, 81, 19, 86}\n",
      "56 -> {67, 3, 37, 12, 13, 79, 29, 62, 63}\n",
      "57 -> {42, 78, 95}\n",
      "58 -> {8, 49, 64}\n",
      "59 -> {33, 49, 46, 9}\n",
      "60 -> {64, 68, 8, 73, 74}\n",
      "61 -> {2, 35, 4, 69, 46, 88}\n",
      "62 -> {0, 50, 51, 54, 27, 29}\n",
      "63 -> {2, 77, 46, 14, 92}\n",
      "64 -> {66, 76, 52, 56, 25}\n",
      "65 -> {66, 11, 17, 83, 29}\n",
      "66 -> {5, 71, 14, 23, 89, 94, 63}\n",
      "67 -> {97, 6, 71, 85, 86, 24, 57}\n",
      "68 -> {16, 75, 80}\n",
      "69 -> {64, 65, 8, 47, 48, 20, 54, 56, 88, 93, 31}\n",
      "70 -> {20, 46, 55}\n",
      "71 -> {72, 10, 17, 18, 54, 89, 59}\n",
      "72 -> {5, 40, 73, 84, 28}\n",
      "73 -> {87}\n",
      "74 -> {41, 47, 79}\n",
      "75 -> {56, 24, 87}\n",
      "76 -> {86, 99, 53, 70}\n",
      "77 -> {41, 75, 85, 59, 61, 62}\n",
      "78 -> {88, 19}\n",
      "79 -> {10, 22, 71, 7}\n",
      "80 -> {56, 42, 91}\n",
      "81 -> {33, 45}\n",
      "82 -> {96, 3, 11, 78, 50, 53, 27, 60, 63}\n",
      "83 -> {18, 60, 38}\n",
      "84 -> {65, 71, 40, 43, 15, 89, 58, 27}\n",
      "85 -> {8, 26, 78, 14}\n",
      "86 -> {11, 45, 79, 21, 86, 23}\n",
      "87 -> {43, 68, 79}\n",
      "88 -> {32, 65, 66, 36, 70, 9, 74, 84, 87, 92}\n",
      "89 -> {0, 41, 14, 47}\n",
      "90 -> {37, 11, 50, 51, 95}\n",
      "91 -> {89, 33, 66, 99, 39, 92, 16, 56, 25, 28, 61, 94, 31}\n",
      "92 -> {81, 19, 94}\n",
      "93 -> {40, 64}\n",
      "94 -> {73, 67, 86}\n",
      "95 -> {6, 49, 52, 90, 30, 95}\n",
      "96 -> {0, 3, 8, 21, 58}\n",
      "97 -> {32, 80, 13}\n",
      "98 -> {65, 36, 8, 43, 49, 81, 22}\n",
      "99 -> {5, 43, 82, 22, 55, 89, 61, 62}\n",
      "100 -> {64, 68, 5, 72, 31}\n",
      "101 -> {77, 15, 82, 25, 26, 29}\n",
      "102 -> {2, 39, 71, 47, 17, 85, 60}\n",
      "103 -> {66, 12, 76, 50, 82, 93}\n",
      "104 -> {34, 36, 40, 42, 50, 87}\n",
      "105 -> {76, 21, 87}\n",
      "106 -> {32, 44, 83, 22, 62}\n",
      "107 -> {81, 91, 12, 77}\n",
      "108 -> {29}\n",
      "109 -> {32, 37, 25, 57, 63}\n",
      "110 -> {13, 67, 45, 70}\n",
      "111 -> {64, 36, 4, 7, 41, 48, 21, 86, 55, 95}\n",
      "112 -> {0, 1, 55}\n",
      "113 -> {33, 34, 77, 51, 61}\n",
      "114 -> {99, 35, 48, 20, 88, 28, 30}\n",
      "115 -> {73, 75, 12, 49, 89}\n",
      "116 -> {29, 51, 76, 13}\n",
      "117 -> {24, 50}\n",
      "118 -> {75, 12, 46}\n",
      "119 -> {78, 3, 6}\n",
      "120 -> {38, 39, 60, 28, 30}\n",
      "121 -> {76, 80, 18, 26, 31}\n",
      "122 -> {80, 49, 40}\n",
      "123 -> {1, 5, 38, 78, 14, 82, 85, 92}\n",
      "124 -> {64, 33, 65, 36, 38, 44, 26}\n",
      "125 -> {43, 52, 69, 14}\n",
      "126 -> {34, 50}\n",
      "127 -> {65, 97, 39, 77, 13, 18, 19, 89, 91, 63}\n",
      "128 -> {7, 39, 54, 89, 58, 94}\n",
      "129 -> {3, 4, 69, 19, 52, 85, 60}\n",
      "130 -> {65, 34, 81, 18, 19, 51, 17}\n",
      "131 -> {38, 39, 9, 15, 59}\n",
      "132 -> {98, 77, 46, 93, 63}\n",
      "133 -> {66, 21, 69}\n",
      "134 -> {24, 2, 88, 42}\n",
      "135 -> {33}\n",
      "136 -> {16, 25, 28, 95}\n",
      "137 -> {96, 8, 61, 30}\n",
      "138 -> {74, 68, 45}\n",
      "139 -> {0, 90, 40, 92}\n",
      "140 -> {73, 29, 30}\n",
      "141 -> {64, 1, 6, 39, 49, 29, 30}\n",
      "142 -> {97, 38, 45, 19, 20, 63}\n",
      "143 -> {68, 10, 15, 49, 59, 61}\n",
      "144 -> {1, 30, 22, 55}\n",
      "145 -> {32, 3, 77, 14, 50}\n",
      "146 -> {96, 12, 95, 79}\n",
      "147 -> {1, 42, 84, 25, 31}\n",
      "148 -> {34, 99, 71, 7, 45}\n",
      "149 -> {17, 50, 11}\n",
      "150 -> {6, 42, 23, 57, 90}\n",
      "151 -> {56, 42}\n",
      "152 -> {1, 10, 53, 25, 28}\n",
      "153 -> {68, 4, 70, 15, 47, 82, 20, 85, 60}\n",
      "154 -> {67, 6, 91, 76, 82, 24, 90, 88}\n",
      "155 -> {72, 84, 46}\n",
      "156 -> {70, 54, 87, 59, 30}\n",
      "157 -> {0, 37, 41, 43, 53, 57, 27}\n",
      "158 -> {32, 6, 78, 54, 55, 95}\n",
      "159 -> {4, 13, 20, 55, 91, 28}\n",
      "160 -> {48, 59}\n",
      "161 -> {0, 4, 68, 7, 12, 53, 24, 90}\n",
      "162 -> {67, 69, 38, 44, 13}\n",
      "163 -> {39, 11, 44, 75, 54, 56, 58}\n",
      "164 -> {70, 8, 21, 24, 63}\n",
      "165 -> {97, 28, 46, 17}\n",
      "166 -> {96, 35, 5, 72, 41, 11, 49, 18, 83, 25, 26, 95}\n",
      "167 -> {97, 2, 4, 25, 91}\n",
      "168 -> {45, 78, 80, 88, 61}\n",
      "169 -> {11, 84, 85, 21, 62}\n",
      "170 -> {4, 90, 41, 74, 44, 47, 16, 18, 21, 22, 86, 57, 58, 91, 62, 31}\n",
      "171 -> {98, 35, 39, 40, 59}\n",
      "172 -> {96, 35, 8, 40, 77, 51, 53, 92}\n",
      "173 -> {9, 58, 74, 53}\n",
      "174 -> {0, 49, 12, 37}\n",
      "175 -> {33}\n",
      "176 -> {26, 52, 14, 15}\n",
      "177 -> {79, 48, 18, 56, 93, 31}\n",
      "178 -> {43, 76}\n",
      "179 -> {27, 48, 52, 88, 89, 91, 60, 30, 57}\n",
      "180 -> {72, 10, 44, 22, 94}\n",
      "181 -> {35, 34, 43}\n",
      "182 -> {69, 93, 87}\n",
      "183 -> {3, 4, 75, 83, 85, 62}\n",
      "184 -> {2, 7, 77, 45, 89, 92}\n",
      "185 -> {90, 68, 54}\n",
      "186 -> {82, 84, 53, 27, 95}\n",
      "187 -> {9, 60, 95, 15}\n",
      "188 -> {57, 98, 94}\n",
      "189 -> {64, 66, 35, 99, 79, 24, 27, 61}\n",
      "190 -> {42, 43, 12, 52, 93}\n",
      "191 -> {1, 98, 10, 78, 59, 93}\n",
      "192 -> {98, 19}\n",
      "193 -> {97, 99, 35, 5, 70, 41, 74, 78, 26}\n",
      "194 -> {19, 44, 61, 68}\n",
      "195 -> {20}\n",
      "196 -> {32, 36, 74, 17, 19, 83, 85, 22}\n",
      "197 -> {32, 81, 66, 14}\n",
      "198 -> {65, 4, 69, 71, 17, 58}\n",
      "199 -> {96, 51, 15}\n"
     ]
    }
   ],
   "source": [
    "num_bands = 10\n",
    "num_buckets = 200\n",
    "def split_vec_and_hash(row):\n",
    "    row = list(row)\n",
    "    size = len(row)//num_bands\n",
    "    return [hash(tuple(row[i*size:(i+1)*size]))%num_buckets for i in range(num_bands)]\n",
    "\n",
    "\n",
    "split_signatures = signed_matrixes.apply(split_vec_and_hash,axis=0)\n",
    "\n",
    "print(split_signatures)\n",
    "\n",
    "buckets = {}\n",
    "\n",
    "def find_collision(column):\n",
    "    for i in range(len(column)):\n",
    "        v = column[i]\n",
    "        if v not in buckets:\n",
    "            buckets[v] = set()\n",
    "            # for each of the elements already in the bucket, add yourself to their nearest neighbors list\n",
    "        buckets[v].add(i)\n",
    "\n",
    "\n",
    "split_signatures.apply(find_collision,axis=1)\n",
    "\n",
    "for i in range(num_buckets):\n",
    "    if i in buckets:\n",
    "        print(f\"{i} -> {buckets[i]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# find nearest neighbours, meaning for each index find if any other index is in the same bucket\n",
    "\n",
    "# calculate jaccard similarity on binary matrix for all nearest neighbours\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hash signature bands into buckets. Find a way to combine all the  signature values in a band and hash them into a number of buckets ususally very high. (10 points)\n",
    "* Easiest way is to add all the signature values in the bucket and use a similar hash function like before\n",
    "* You should use the same hash function for all bands. And all documents ending up in same bucket for at least one band are considered as candidate pairs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tune parameters to make sure the threshold is appropriate. (10 points)\n",
    "* plot the probability of two similar items falling in same bucket for different threshold values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Choose the best parameters and get nearest neighbors of each articles (20 points)\n",
    "* Jaccard Similarity\n",
    "* convert hash table into dictionary of article ids and its other articles that hashed in at least 1 same bucket\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Write the nearest neibhors of each document to submissions.csv (comma separated, first column is the current document followed by a list of nearest neighbors) file and get the score (10 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Write a report + notebook + submission file in a zip file (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjK4_fGbHYIA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coy5Lxd6JcaI"
   },
   "outputs": [],
   "source": [
    "def getFrequentNgrams(articles):\n",
    "  # Your code Here\n",
    "    # Select most frequent n-grams\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaAvfYIkJvYf"
   },
   "outputs": [],
   "source": [
    "def getBinaryMatrix(docs):\n",
    "  # Your code Here\n",
    "       # return binary_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX7R93sYMoM0"
   },
   "outputs": [],
   "source": [
    "def getHashFunctionValues(numrows, numhashfunctions):\n",
    "    # Your code Here\n",
    "    #return a matrix with hash values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trWgSifwNEQ2"
   },
   "outputs": [],
   "source": [
    "def getMinHashSignatureMatrix(binary_matrix, hash_val_matrix):\n",
    "    #return minhash signature matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRC-FWUSSAON"
   },
   "outputs": [],
   "source": [
    "def getLSH(signature_matrix, num_bands, num_buckets):\n",
    "    #return lsh buckets or hash table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNmDBSVuSdvV"
   },
   "outputs": [],
   "source": [
    "def plotProbability(s, b, r):\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYqQg7BfVPuN"
   },
   "outputs": [],
   "source": [
    "def getJaccardSimilarityScore(C1, C2):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qzcoHoGVasE"
   },
   "outputs": [],
   "source": [
    "# convert hash table into dictionary of article ids and its other articles that hashed in at least 1 same bucket\n",
    "nearest_neighbors = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkEBs1OOWbuZ"
   },
   "outputs": [],
   "source": [
    "# Remove the neighbors in same buckets but have similarity score < threshold s\n",
    "n_copy = copy.deepcopy(nearest_neighbors)\n",
    "submission_id = []\n",
    "submission_nid = []\n",
    "for article_id, neighbor_ids in n_copy.items():\n",
    "    for nid in neighbor_ids:\n",
    "        score = \n",
    "        if score < s:\n",
    "           \n",
    "        else:\n",
    "            # add to submission result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wivKramXf-1"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['article_id'] = submission_id\n",
    "data['neighbor_id'] = submission_nid\n",
    "data.sort_values(by=['article_id', 'neighbor_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpqOKfR5Xlqz"
   },
   "outputs": [],
   "source": [
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fzmz-qQxXpA6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python-dat550 kernel",
   "language": "python",
   "name": "python-dat550"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
